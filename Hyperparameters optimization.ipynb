{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier \n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "import optuna\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train_folds.csv\")\n",
    "test = pd.read_csv(\"Test.csv\")\n",
    "sample = pd.read_csv(\"SampleSubmission.csv\")\n",
    "\n",
    "\n",
    "useful_features = [i for i in train.columns if i not in(\"region_area_\", \"Potability\", \"kfold\")]\n",
    "test = test[useful_features]\n",
    "\n",
    "for col in test.columns:\n",
    "    train[col] = np.log1p(train[col])\n",
    "test[col] = np.log1p(test[col])\n",
    "\n",
    "def run(trial):\n",
    "            fold=0\n",
    "            learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n",
    "            reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n",
    "            reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n",
    "            sub_sample = trial.suggest_float(\"sub_sample\", 0.1, 1.0)\n",
    "            max_depth = trial.suggest_int(\"max_depth\", 1,7)\n",
    "            colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n",
    "\n",
    "            x_train = train[train.kfold != fold].reset_index(drop=True)\n",
    "            x_valid = train[train.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "            y_train = x_train.Potability\n",
    "            y_valid = x_valid.Potability\n",
    "\n",
    "            x_train = x_train[useful_features]\n",
    "            x_valid = x_valid[useful_features]\n",
    "\n",
    "            sc = preprocessing.Normalizer()\n",
    "            scaled_x_train = pd.DataFrame(sc.fit_transform(x_train))\n",
    "            scaled_x_valid = pd.DataFrame(sc.transform(x_valid))\n",
    "            #scaled_test = pd.DataFrame(sc.transform(df_test))\n",
    "\n",
    "            scaled_x_train.columns = x_train.columns\n",
    "            scaled_x_valid.columns = x_valid.columns\n",
    "            #scaled_test.columns = df_test.columns\n",
    "\n",
    "\n",
    "            model = XGBClassifier(random_state=42,\n",
    "                n_estimators=7000,\n",
    "                learning_rate = learning_rate,\n",
    "                reg_lambda = reg_lambda,\n",
    "                reg_alpha = reg_alpha,\n",
    "                sub_sample = sub_sample,\n",
    "                max_depth = max_depth,\n",
    "                colsample_bytree = colsample_bytree,\n",
    "             )\n",
    "            model.fit(scaled_x_train, y_train)\n",
    "            preds = model.predict(scaled_x_valid)\n",
    "            score = model.score(scaled_x_valid, y_valid)\n",
    "            #score = model.score(scaled_x_train, y_train)\n",
    "        \n",
    "            return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-22 07:32:26,301]\u001b[0m A new study created in memory with name: no-name-64c249e7-4841-40e9-9022-aa59fbb79bcd\u001b[0m\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:32:26] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { sub_sample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[07:32:26] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-22 07:32:38,373]\u001b[0m Trial 0 finished with value: 0.6482300884955752 and parameters: {'learning_rate': 0.18575843133189782, 'reg_lambda': 28.40017935663664, 'reg_alpha': 0.0024694384631797654, 'sub_sample': 0.9772611014683914, 'max_depth': 6, 'colsample_bytree': 0.28508665839493885}. Best is trial 0 with value: 0.6482300884955752.\u001b[0m\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:32:38] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { sub_sample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[07:32:38] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-22 07:32:49,030]\u001b[0m Trial 1 finished with value: 0.6150442477876106 and parameters: {'learning_rate': 0.05793570035650512, 'reg_lambda': 5.655237322849009e-07, 'reg_alpha': 3.125582722504243e-07, 'sub_sample': 0.9726727032735669, 'max_depth': 4, 'colsample_bytree': 0.14034063960129942}. Best is trial 0 with value: 0.6482300884955752.\u001b[0m\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:32:49] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { sub_sample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[07:32:49] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-22 07:33:01,752]\u001b[0m Trial 2 finished with value: 0.661504424778761 and parameters: {'learning_rate': 0.04569642602233985, 'reg_lambda': 0.5775506658770446, 'reg_alpha': 2.7447735238679504e-07, 'sub_sample': 0.13633748376033483, 'max_depth': 7, 'colsample_bytree': 0.4198261718388938}. Best is trial 2 with value: 0.661504424778761.\u001b[0m\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:33:01] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { sub_sample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[07:33:01] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-22 07:33:17,101]\u001b[0m Trial 3 finished with value: 0.584070796460177 and parameters: {'learning_rate': 0.07966649103206408, 'reg_lambda': 18.936054828839115, 'reg_alpha': 2.4861953594811386e-05, 'sub_sample': 0.4253022009777645, 'max_depth': 6, 'colsample_bytree': 0.11092049950389965}. Best is trial 2 with value: 0.661504424778761.\u001b[0m\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:33:17] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { sub_sample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[07:33:17] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-22 07:33:22,773]\u001b[0m Trial 4 finished with value: 0.6415929203539823 and parameters: {'learning_rate': 0.016502208837650974, 'reg_lambda': 0.013599644757561705, 'reg_alpha': 65.92024764142482, 'sub_sample': 0.7194132110802006, 'max_depth': 6, 'colsample_bytree': 0.5612523460496128}. Best is trial 2 with value: 0.661504424778761.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(run,n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.04569642602233985,\n",
       " 'reg_lambda': 0.5775506658770446,\n",
       " 'reg_alpha': 2.7447735238679504e-07,\n",
       " 'sub_sample': 0.13633748376033483,\n",
       " 'max_depth': 7,\n",
       " 'colsample_bytree': 0.4198261718388938}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with 5 folds\n",
    "{'learning_rate': 0.058937452875490244,\n",
    " 'reg_lambda': 0.27685962282866644,\n",
    " 'reg_alpha': 5.1246361644341025,\n",
    " 'sub_sample': 0.7649184695897516,\n",
    " 'max_depth': 2,\n",
    " 'colsample_bytree': 0.7450191595678853}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'learning_rate': 0.04569642602233985,\n",
    " 'reg_lambda': 0.5775506658770446,\n",
    " 'reg_alpha': 2.7447735238679504e-07,\n",
    " 'sub_sample': 0.13633748376033483,\n",
    " 'max_depth': 7,\n",
    " 'colsample_bytree': 0.4198261718388938}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
